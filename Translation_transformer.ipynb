{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niko55/Deep-Learning/blob/master/Translation_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvIKYOJff-1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahH7o2QOgpJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YGI2kz4L3bI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa417824-6377-4a59-a8b8-3a8e5940961f"
      },
      "source": [
        "print(examples['train'])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.string)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2KxeW8ogpLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus((en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "#print(tokenizer_en)\n",
        "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus((pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "#print(tokenizer_pt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQxf2mrygpOA",
        "colab_type": "code",
        "outputId": "99e5baeb-f792-49ce-add4-369798960325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sample_string = 'Transformer is awesome.'\n",
        "\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "print ('Tokenized string is post encoding {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_en.decode(tokenized_string)\n",
        "print ('The original string post decoding: {}'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized string is post encoding [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
            "The original string post decoding: Transformer is awesome.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsWXLPbLgpQc",
        "colab_type": "code",
        "outputId": "f183f9b9-b4c2-40b5-e6ca-f9399a5fb589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7915 ----> T\n",
            "1248 ----> ran\n",
            "7946 ----> s\n",
            "7194 ----> former \n",
            "13 ----> is \n",
            "2799 ----> awesome\n",
            "7877 ----> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozxC3T8pirsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nRgTl2Eishf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "  return lang1, lang2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UyxKTxCisjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13QoPFkUislz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,tf.size(y) <= max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaXEcg_EisnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_encode(pt, en):\n",
        "  return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnwMlPgFisrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length).padded_batch(\n",
        "    BATCH_SIZE, padded_shapes=([-1], [-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkXye4eZjWqm",
        "colab_type": "code",
        "outputId": "d33f05b1-1e8c-48c7-c7b9-fee5eef10928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "pt_batch, en_batch = next(iter(val_dataset))\n",
        "pt_batch, en_batch"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=9034907, shape=(64, 40), dtype=int64, numpy=\n",
              " array([[8214, 1259,    5, ...,    0,    0,    0],\n",
              "        [8214,  299,   13, ...,    0,    0,    0],\n",
              "        [8214,   59,    8, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8214,   95,    3, ...,    0,    0,    0],\n",
              "        [8214, 5157,    1, ...,    0,    0,    0],\n",
              "        [8214, 4479, 7990, ...,    0,    0,    0]])>,\n",
              " <tf.Tensor: id=9034908, shape=(64, 40), dtype=int64, numpy=\n",
              " array([[8087,   18,   12, ...,    0,    0,    0],\n",
              "        [8087,  634,   30, ...,    0,    0,    0],\n",
              "        [8087,   16,   13, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8087,   12,   20, ...,    0,    0,    0],\n",
              "        [8087,   17, 4981, ...,    0,    0,    0],\n",
              "        [8087,   12, 5453, ...,    0,    0,    0]])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX20pTNUlVzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLEAMtPclWly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0W_TB7xlaOy",
        "colab_type": "code",
        "outputId": "f5acd5c0-4e4f-49ca-a33c-09d54c0c3fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nR2TEdZliAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAfSNQQylaRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfbb3hf9laUM",
        "colab_type": "code",
        "outputId": "5e12c2a1-eec9-4db4-e400-c632ef7114f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=9034922, shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQevkSDFlaWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7ZOTJpel1FI",
        "colab_type": "code",
        "outputId": "b4ce9e99-0042-4b29-a0ff-d2686c5545f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=9034937, shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBN3ylm5l4ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E56jpNCl4c2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urGrrh_Pl4fV",
        "colab_type": "code",
        "outputId": "39873c3b-22e0-4222-8713-df5182c3f662",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43nXAdcIl4iI",
        "colab_type": "code",
        "outputId": "fb1bdacd-d397-4c3e-dd62-5dfdc86a1e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This query aligns with a repeated key (third and fourth), \n",
        "# so all associated values get averaged.\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD6ysR3xmEXK",
        "colab_type": "code",
        "outputId": "ef6d6b97-5955-434d-c205-539ec9ee9385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This query aligns equally with the first and second key, \n",
        "# so their values get averaged.\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkVrFvIWmEZl",
        "colab_type": "code",
        "outputId": "1208ce01-195d-482a-f6b5-e8841b1364a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor(\n",
            "[[0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor(\n",
            "[[550.    5.5]\n",
            " [ 10.    0. ]\n",
            " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efJkFK8qmEcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJoqDMoGmEez",
        "colab_type": "code",
        "outputId": "227d24cf-62e4-4286-f534-f74dac0d367a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(1), Dimension(60), Dimension(512)]),\n",
              " TensorShape([Dimension(1), Dimension(8), Dimension(60), Dimension(60)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-4F6-fmmEht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "      ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nvfgRyKmcXC",
        "colab_type": "code",
        "outputId": "68177c16-1230-421d-8add-a36a32bd5b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(64), Dimension(50), Dimension(512)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82Ifdvj4mcZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlnWR0RfmccT",
        "colab_type": "code",
        "outputId": "844cf3bb-9782-4ecb-9f05-33afaad7559c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(64), Dimension(43), Dimension(512)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwcdH4wcmceL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUt13Hq5mmCa",
        "colab_type": "code",
        "outputId": "d34996eb-b49d-411d-f6a4-bbb0c7aaf57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(64), Dimension(50), Dimension(512)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R8n3dAlmmFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE72itt2mt9G",
        "colab_type": "code",
        "outputId": "2c6bfbff-bf9f-43b3-c6bf-f11085e569b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwDbtdh2mt_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lonqhheKmuCT",
        "colab_type": "code",
        "outputId": "5e39994f-522a-44ec-9f80-563fdacfef95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input, \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False,\n",
        "                              look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([Dimension(64), Dimension(26), Dimension(512)]),\n",
              " TensorShape([Dimension(64), Dimension(8), Dimension(26), Dimension(62)]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HixxIIa4m8i9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Z76NG3m8l-",
        "colab_type": "code",
        "outputId": "8a1a34a9-393a-4a93-a83c-1a30ffed81bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500, target_vocab_size=8000, \n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(64), Dimension(36), Dimension(8000)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sv5behSm8o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
        "target_vocab_size = tokenizer_en.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_ecPl9_m8sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMSWmIhfnFrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT7XcrwWnFuI",
        "colab_type": "code",
        "outputId": "bb2b9786-1592-4bd2-87c5-81209a86deb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "'''\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")\n",
        "'''"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\\nplt.ylabel(\"Learning Rate\")\\nplt.xlabel(\"Train Step\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSaxpkwLnFwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sncJj5e7m8rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh-gbCA1nmJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9UxJlp7nmLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOubXrYInmOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zhrZvUln31i",
        "colab_type": "code",
        "outputId": "4f7dd5a6-7359-4f6f-c405-7b6e505d80a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6J-uG3ynmQ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61e14c7b-9acb-4ebd-e21b-eb30c3a36e62"
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90sPV99EoP6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3LiayMDoP_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lfUMHMmoQCf",
        "colab_type": "code",
        "outputId": "8f58ff05-7070-4659-9cc2-8b098672ac86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.4061 Accuracy 0.3864\n",
            "Epoch 1 Batch 50 Loss 0.3377 Accuracy 0.3839\n",
            "Epoch 1 Batch 100 Loss 0.3424 Accuracy 0.3858\n",
            "Epoch 1 Batch 150 Loss 0.3424 Accuracy 0.3856\n",
            "Epoch 1 Batch 200 Loss 0.3460 Accuracy 0.3850\n",
            "Epoch 1 Batch 250 Loss 0.3451 Accuracy 0.3838\n",
            "Epoch 1 Batch 300 Loss 0.3474 Accuracy 0.3826\n",
            "Epoch 1 Batch 350 Loss 0.3484 Accuracy 0.3814\n",
            "Epoch 1 Batch 400 Loss 0.3501 Accuracy 0.3812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFXvA118oY4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  start_token = [tokenizer_pt.vocab_size]\n",
        "  end_token = [tokenizer_pt.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input = [tokenizer_en.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_en.vocab_size+1:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuVnKYHYoY79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "  sentence = tokenizer_pt.encode(sentence)\n",
        "  \n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    # plot the attention weights\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 10}\n",
        "    \n",
        "    ax.set_xticks(range(len(sentence)+2))\n",
        "    ax.set_yticks(range(len(result)))\n",
        "    \n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "    ax.set_xticklabels(\n",
        "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "        fontdict=fontdict, rotation=90)\n",
        "    \n",
        "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                        if i < tokenizer_en.vocab_size], \n",
        "                       fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY6ORozEoY-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "  \n",
        "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
        "                                            if i < tokenizer_en.vocab_size])  \n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWoc4PcroZB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
        "print (\"Real translation: and my neighboring homes heard about this idea .\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMvCFpuxoxD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"este é um problema que temos que resolver.\")\n",
        "print (\"Real translation: this is a problem we have to solve .\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPQ8lU2moxG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
        "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDcrCyLjoxKD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOlXHIspAu2D",
        "colab_type": "text"
      },
      "source": [
        "Transformer model for Aspect based Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z46Ay5-eAzqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPfF17LKAzvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGY7H9E4Azt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}