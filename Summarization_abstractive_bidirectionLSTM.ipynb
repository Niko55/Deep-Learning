{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niko55/Deep-Learning/blob/master/Summarization_abstractive_bidirectionLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ4zP5oSHSbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import csv\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "import json, gzip\n",
        "from pandas.io.json import json_normalize\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfAEhH6UHh7q",
        "colab_type": "text"
      },
      "source": [
        "Loading PreProcessed Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDk15Wd1Hn6n",
        "colab_type": "code",
        "outputId": "704324b8-3001-49cf-f846-d976ffbd5698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PavV1YtKHg0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/Data/release/processed.json') as file:\n",
        "\n",
        "    for json_data in file:\n",
        "        saved_data = json.loads(json_data)\n",
        "\n",
        "        vocab2idx = saved_data[\"vocab\"]\n",
        "        embd = saved_data[\"embd\"]\n",
        "        train_batches_text = saved_data[\"train_batches_text\"]\n",
        "        test_batches_text = saved_data[\"test_batches_text\"]\n",
        "        val_batches_text = saved_data[\"val_batches_text\"]\n",
        "        train_batches_summary = saved_data[\"train_batches_summary\"]\n",
        "        test_batches_summary = saved_data[\"test_batches_summary\"]\n",
        "        val_batches_summary = saved_data[\"val_batches_summary\"]\n",
        "        train_batches_true_text_len = saved_data[\"train_batches_true_text_len\"]\n",
        "        val_batches_true_text_len = saved_data[\"val_batches_true_text_len\"]\n",
        "        test_batches_true_text_len = saved_data[\"test_batches_true_text_len\"]\n",
        "        train_batches_true_summary_len = saved_data[\"train_batches_true_summary_len\"]\n",
        "        val_batches_true_summary_len = saved_data[\"val_batches_true_summary_len\"]\n",
        "        test_batches_true_summary_len = saved_data[\"test_batches_true_summary_len\"]\n",
        "\n",
        "        break\n",
        "        \n",
        "idx2vocab = {v:k for k,v in vocab2idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgEo7VRCIAbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#HyperParameters\n",
        "hidden_size = 300\n",
        "learning_rate = 0.001\n",
        "epochs = 5\n",
        "max_summary_len = 101 # should be summary_max_len as used in data_preprocessing with +1 (+1 for <EOS>) \n",
        "D = 5 # D determines local attention window size\n",
        "window_len = 2*D+1\n",
        "l2=1e-6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7EPzy1Q4GJb",
        "colab_type": "text"
      },
      "source": [
        "**Tensorflow Placeholders (tensorflow 2.0)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fArdq1gg0SXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "tf.reset_default_graph()\n",
        "\n",
        "embd_dim = len(embd[0])\n",
        "\n",
        "tf_text = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
        "tf_embd = tf.compat.v1.placeholder(tf.float32, [len(vocab2idx), embd_dim])\n",
        "tf_true_summary_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
        "tf_summary = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
        "tf_train = tf.compat.v1.placeholder(tf.bool)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of_4ZLK2Msx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Embed vectorize text\n",
        "embd_text = tf.nn.embedding_lookup(tf_embd, tf_text)\n",
        "embd_text = tf.layers.dropout(embd_text,rate=0.3,training=tf_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPrC_rAMM3AQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM function\n",
        "def LSTM(x,hidden_state,cell,input_dim,hidden_size,scope):\n",
        "    \n",
        "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
        "        \n",
        "        w = tf.get_variable(\"w\", shape=[4,input_dim,hidden_size],\n",
        "                                    dtype=tf.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=tf.glorot_uniform_initializer())\n",
        "        \n",
        "        u = tf.get_variable(\"u\", shape=[4,hidden_size,hidden_size],\n",
        "                            dtype=tf.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=tf.glorot_uniform_initializer())\n",
        "        \n",
        "        b = tf.get_variable(\"bias\", shape=[4,1,hidden_size],\n",
        "                    dtype=tf.float32,\n",
        "                    trainable=True,\n",
        "                    initializer=tf.zeros_initializer())\n",
        "        \n",
        "    input_gate = tf.nn.sigmoid( tf.matmul(x,w[0]) + tf.matmul(hidden_state,u[0]) + b[0])\n",
        "    forget_gate = tf.nn.sigmoid( tf.matmul(x,w[1]) + tf.matmul(hidden_state,u[1]) + b[1])\n",
        "    output_gate = tf.nn.sigmoid( tf.matmul(x,w[2]) + tf.matmul(hidden_state,u[2]) + b[2])\n",
        "    cell_ = tf.nn.tanh( tf.matmul(x,w[3]) + tf.matmul(hidden_state,u[3]) + b[3])\n",
        "    cell = forget_gate*cell + input_gate*cell_\n",
        "    hidden_state = output_gate*tf.tanh(cell)\n",
        "    \n",
        "    return hidden_state, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wNEVvfJM6XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FORWARD ENCODING\n",
        "S = tf.shape(embd_text)[1] #text sequence length\n",
        "N = tf.shape(embd_text)[0] #batch_size\n",
        "\n",
        "i=0\n",
        "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
        "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
        "hidden_forward=tf.TensorArray(size=S, dtype=tf.float32)\n",
        "\n",
        "#shape of embd_text: [N,S,embd_dim]\n",
        "embd_text_t = tf.transpose(embd_text,[1,0,2]) \n",
        "#current shape of embd_text: [S,N,embd_dim]\n",
        "\n",
        "def cond(i, hidden, cell, hidden_forward):\n",
        "    return i < S\n",
        "\n",
        "def body(i, hidden, cell, hidden_forward):\n",
        "    x = embd_text_t[i]\n",
        "    \n",
        "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"forward_encoder\")\n",
        "    hidden_forward = hidden_forward.write(i, hidden)\n",
        "\n",
        "    return i+1, hidden, cell, hidden_forward\n",
        "\n",
        "_, _, _, hidden_forward = tf.while_loop(cond, body, [i, hidden, cell, hidden_forward])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk87wS-LNHnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Backward Encoding\n",
        "i=S-1\n",
        "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
        "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
        "hidden_backward=tf.TensorArray(size=S, dtype=tf.float32)\n",
        "\n",
        "def cond(i, hidden, cell, hidden_backward):\n",
        "    return i >= 0\n",
        "\n",
        "def body(i, hidden, cell, hidden_backward):\n",
        "    x = embd_text_t[i]\n",
        "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"backward_encoder\")\n",
        "    hidden_backward = hidden_backward.write(i, hidden)\n",
        "\n",
        "    return i-1, hidden, cell, hidden_backward\n",
        "\n",
        "_, _, _, hidden_backward = tf.while_loop(cond, body, [i, hidden, cell, hidden_backward])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0BLKAniNOLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Merge Forward and Backward Encoder Hidden States\n",
        "hidden_forward = hidden_forward.stack()\n",
        "hidden_backward = hidden_backward.stack()\n",
        "\n",
        "encoder_states = tf.concat([hidden_forward,hidden_backward],axis=-1)\n",
        "encoder_states = tf.transpose(encoder_states,[1,0,2])\n",
        "\n",
        "encoder_states = tf.layers.dropout(encoder_states,rate=0.3,training=tf_train)\n",
        "\n",
        "final_encoded_state = tf.layers.dropout(tf.concat([hidden_forward[-1],hidden_backward[-1]],axis=-1),rate=0.3,training=tf_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLS9ICfiNYsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Implementation of Scoring function\n",
        "def attention_score(encoder_states,decoder_hidden_state,scope=\"attention_score\"):\n",
        "    \n",
        "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
        "        Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size],\n",
        "                                    dtype=tf.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=tf.glorot_uniform_initializer())\n",
        "        \n",
        "    encoder_states = tf.reshape(encoder_states,[N*S,2*hidden_size])\n",
        "    \n",
        "    encoder_states = tf.reshape(tf.matmul(encoder_states,Wa),[N,S,2*hidden_size])\n",
        "    decoder_hidden_state = tf.reshape(decoder_hidden_state,[N,2*hidden_size,1])\n",
        "    \n",
        "    return tf.reshape(tf.matmul(encoder_states,decoder_hidden_state),[N,S])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Acl8bT5NdqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Local Attention Function\n",
        "def align(encoder_states, decoder_hidden_state,scope=\"attention\"):\n",
        "    \n",
        "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
        "        Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,125],\n",
        "                                    dtype=tf.float32,\n",
        "                                    trainable=True,\n",
        "                                    initializer=tf.glorot_uniform_initializer())\n",
        "        \n",
        "        Vp = tf.get_variable(\"Vp\", shape=[125,1],\n",
        "                            dtype=tf.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=tf.glorot_uniform_initializer())\n",
        "    \n",
        "    positions = tf.cast(S-window_len,dtype=tf.float32) # Maximum valid attention window starting position\n",
        "    \n",
        "    # Predict attention window starting position \n",
        "    ps = positions*tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(decoder_hidden_state,Wp)),Vp))\n",
        "    # ps = (soft-)predicted starting position of attention window\n",
        "    pt = ps+D # pt = center of attention window where the whole window length is 2*D+1\n",
        "    pt = tf.reshape(pt,[N])\n",
        "    \n",
        "    i = 0\n",
        "    gaussian_position_based_scores = tf.TensorArray(size=S,dtype=tf.float32)\n",
        "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
        "    \n",
        "    def cond(i,gaussian_position_based_scores):\n",
        "        \n",
        "        return i < S\n",
        "                      \n",
        "    def body(i,gaussian_position_based_scores):\n",
        "        \n",
        "        score = tf.exp(-((tf.square(tf.cast(i,tf.float32)-pt))/(2*tf.square(sigma)))) \n",
        "        # (equation (10) in https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)\n",
        "        gaussian_position_based_scores = gaussian_position_based_scores.write(i,score)\n",
        "            \n",
        "        return i+1,gaussian_position_based_scores\n",
        "                      \n",
        "    i,gaussian_position_based_scores = tf.while_loop(cond,body,[i,gaussian_position_based_scores])\n",
        "    \n",
        "    gaussian_position_based_scores = gaussian_position_based_scores.stack()\n",
        "    gaussian_position_based_scores = tf.transpose(gaussian_position_based_scores,[1,0])\n",
        "    gaussian_position_based_scores = tf.reshape(gaussian_position_based_scores,[N,S])\n",
        "    \n",
        "    scores = attention_score(encoder_states,decoder_hidden_state)*gaussian_position_based_scores\n",
        "    scores = tf.nn.softmax(scores,axis=-1)\n",
        "    \n",
        "    return tf.reshape(scores,[N,S,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C16FXduRNm6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
        "    SOS = tf.get_variable(\"sos\", shape=[1,embd_dim],\n",
        "                                dtype=tf.float32,\n",
        "                                trainable=True,\n",
        "                                initializer=tf.glorot_uniform_initializer())\n",
        "    \n",
        "    # SOS represents starting marker \n",
        "    # It tells the decoder that it is about to decode the first word of the output\n",
        "    # I have set SOS as a trainable parameter\n",
        "    \n",
        "    Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,embd_dim],\n",
        "                            dtype=tf.float32,\n",
        "                            trainable=True,\n",
        "                            initializer=tf.glorot_uniform_initializer())\n",
        "    \n",
        "\n",
        "\n",
        "SOS = tf.tile(SOS,[N,1]) #now SOS shape: [N,embd_dim]\n",
        "inp = SOS\n",
        "hidden=final_encoded_state\n",
        "cell=tf.zeros([N, 2*hidden_size], dtype=tf.float32)\n",
        "decoder_outputs=tf.TensorArray(size=max_summary_len, dtype=tf.float32)\n",
        "outputs=tf.TensorArray(size=max_summary_len, dtype=tf.int32)\n",
        "\n",
        "for i in range(max_summary_len):\n",
        "    \n",
        "    inp = tf.layers.dropout(inp,rate=0.3,training=tf_train)\n",
        "    \n",
        "    attention_scores = align(encoder_states,hidden)\n",
        "    encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
        "    \n",
        "    hidden,cell = LSTM(inp,hidden,cell,embd_dim,2*hidden_size,scope=\"decoder\")\n",
        "    \n",
        "    hidden_ = tf.layers.dropout(hidden,rate=0.3,training=tf_train)\n",
        "    \n",
        "    concated = tf.concat([hidden_,encoder_context_vector],axis=-1)\n",
        "    \n",
        "    linear_out = tf.nn.tanh(tf.matmul(concated,Wc))\n",
        "    decoder_output = tf.matmul(linear_out,tf.transpose(tf_embd,[1,0])) \n",
        "    # produce unnormalized probability distribution over vocabulary\n",
        "    \n",
        "    \n",
        "    decoder_outputs = decoder_outputs.write(i,decoder_output)\n",
        "    \n",
        "    # Pick out most probable vocab indices based on the unnormalized probability distribution\n",
        "    \n",
        "    next_word_vec = tf.cast(tf.argmax(decoder_output,1),tf.int32)\n",
        "\n",
        "    next_word_vec = tf.reshape(next_word_vec, [N])\n",
        "\n",
        "    outputs = outputs.write(i,next_word_vec)\n",
        "\n",
        "    next_word = tf.nn.embedding_lookup(tf_embd, next_word_vec)\n",
        "    inp = tf.reshape(next_word, [N, embd_dim])\n",
        "    \n",
        "    \n",
        "decoder_outputs = decoder_outputs.stack()\n",
        "outputs = outputs.stack()\n",
        "\n",
        "decoder_outputs = tf.transpose(decoder_outputs,[1,0,2])\n",
        "outputs = tf.transpose(outputs,[1,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwMczFDlNt2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_trainables = [var for var in tf.trainable_variables() if\n",
        "                       not(\"Bias\" in var.name or \"bias\" in var.name\n",
        "                           or \"noreg\" in var.name)]\n",
        "\n",
        "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var\n",
        "                                in filtered_trainables])\n",
        "\n",
        "with tf.variable_scope(\"loss\"):\n",
        "\n",
        "    epsilon = tf.constant(1e-9, tf.float32)\n",
        "\n",
        "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=tf_summary, logits=decoder_outputs)\n",
        "\n",
        "    pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
        "                                maxlen=max_summary_len,\n",
        "                                dtype=tf.float32)\n",
        "\n",
        "    masked_cross_entropy = cross_entropy*pad_mask\n",
        "\n",
        "    cost = tf.reduce_mean(masked_cross_entropy) + \\\n",
        "        l2*regularization\n",
        "\n",
        "    cross_entropy = tf.reduce_mean(masked_cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CdWrjXKNuD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comparing predicted sequence with labels\n",
        "comparison = tf.cast(tf.equal(outputs, tf_summary),\n",
        "                     tf.float32)\n",
        "\n",
        "# Masking to ignore the effect of pads while calculating accuracy\n",
        "pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
        "                            maxlen=max_summary_len,\n",
        "                            dtype=tf.bool)\n",
        "\n",
        "masked_comparison = tf.boolean_mask(comparison, pad_mask)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = tf.reduce_mean(masked_comparison)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWpAb--DN81-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "\n",
        "optimizer = tf.contrib.opt.NadamOptimizer(\n",
        "    learning_rate=learning_rate)\n",
        "\n",
        "gvs = optimizer.compute_gradients(cost, var_list=all_vars)\n",
        "\n",
        "capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs] # Gradient Clipping\n",
        "\n",
        "train_op = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2h9YtIVOPst",
        "colab_type": "code",
        "outputId": "d104e0f6-34ee-4b73-fbe5-3823f31ace8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "with tf.Session() as sess:  # Start Tensorflow Session\n",
        "    display_step = 100\n",
        "    patience = 5\n",
        "\n",
        "    load = input(\"\\nLoad checkpoint? y/n: \")\n",
        "    print(\"\")\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "#opt = tf.keras.optimizers.Adam(0.1)\n",
        "#ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net)\n",
        "\n",
        "    if load.lower() == 'y':\n",
        "\n",
        "        print('Loading pre-trained weights for the model...')\n",
        "        saver.restore(sess, '/content/Model_Backup/Seq2seq_summarization.ckpt')\n",
        "        sess.run(tf.global_variables())\n",
        "        sess.run(tf.tables_initializer())\n",
        "\n",
        "        with open('/content/Model_Backup/Seq2seq_summarization.pkl', 'rb') as fp:\n",
        "            train_data = pickle.load(fp)\n",
        "\n",
        "        covered_epochs = train_data['covered_epochs']\n",
        "        best_loss = train_data['best_loss']\n",
        "        impatience = 0\n",
        "        \n",
        "        print('\\nRESTORATION COMPLETE\\n')\n",
        "\n",
        "    else:\n",
        "        best_loss = 2**30\n",
        "        impatience = 0\n",
        "        covered_epochs = 0\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        sess.run(tf.tables_initializer())\n",
        "\n",
        "    epoch=0\n",
        "    while (epoch+covered_epochs)<epochs:\n",
        "        \n",
        "        print(\"\\n\\nSTARTING TRAINING\\n\\n\")\n",
        "        \n",
        "        batches_indices = [i for i in range(0, len(train_batches_text))]\n",
        "        random.shuffle(batches_indices)\n",
        "\n",
        "        total_train_acc = 0\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for i in range(0, len(train_batches_text)):\n",
        "            \n",
        "            j = int(batches_indices[i])\n",
        "\n",
        "            cost,prediction,\\\n",
        "                acc, _ = sess.run([cross_entropy,\n",
        "                                   outputs,\n",
        "                                   accuracy,\n",
        "                                   train_op],\n",
        "                                  feed_dict={tf_text: train_batches_text[j],\n",
        "                                             tf_embd: embd,\n",
        "                                             tf_summary: train_batches_summary[j],\n",
        "                                             tf_true_summary_len: train_batches_true_summary_len[j],\n",
        "                                             tf_train: True})\n",
        "            \n",
        "            total_train_acc += acc\n",
        "            total_train_loss += cost\n",
        "\n",
        "            if i % display_step == 0:\n",
        "                print(\"Iter \"+str(i)+\", Cost= \" +\n",
        "                      \"{:.3f}\".format(cost)+\", Acc = \" +\n",
        "                      \"{:.2f}%\".format(acc*100))\n",
        "            \n",
        "            if i % 500 == 0:\n",
        "                \n",
        "                idx = random.randint(0,len(train_batches_text[j])-1)\n",
        "                \n",
        "                \n",
        "                \n",
        "                text = \" \".join([idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_text[j][idx]])\n",
        "                predicted_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in prediction[idx]]\n",
        "                actual_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_summary[j][idx]]\n",
        "                \n",
        "                print(\"\\nSample Text\\n\")\n",
        "                print(text)\n",
        "                print(\"\\nSample Predicted Summary\\n\")\n",
        "                for word in predicted_summary:\n",
        "                    if word == '<EOS>':\n",
        "                        break\n",
        "                    else:\n",
        "                        print(word,end=\" \")\n",
        "                print(\"\\n\\nSample Actual Summary\\n\")\n",
        "                for word in actual_summary:\n",
        "                    if word == '<EOS>':\n",
        "                        break\n",
        "                    else:\n",
        "                        print(word,end=\" \")\n",
        "                print(\"\\n\\n\")\n",
        "                \n",
        "        print(\"\\n\\nSTARTING VALIDATION\\n\\n\")\n",
        "                \n",
        "        total_val_loss=0\n",
        "        total_val_acc=0\n",
        "                \n",
        "        for i in range(0, len(val_batches_text)):\n",
        "            \n",
        "            if i%100==0:\n",
        "                print(\"Validating data # {}\".format(i))\n",
        "\n",
        "            cost, prediction,\\\n",
        "                acc = sess.run([cross_entropy,\n",
        "                                outputs,\n",
        "                                accuracy],\n",
        "                                  feed_dict={tf_text: val_batches_text[i],\n",
        "                                             tf_embd: embd,\n",
        "                                             tf_summary: val_batches_summary[i],\n",
        "                                             tf_true_summary_len: val_batches_true_summary_len[i],\n",
        "                                             tf_train: False})\n",
        "            \n",
        "            total_val_loss += cost\n",
        "            total_val_acc += acc\n",
        "            \n",
        "        avg_val_loss = total_val_loss/len(val_batches_text)\n",
        "        \n",
        "        print(\"\\n\\nEpoch: {}\\n\\n\".format(epoch+covered_epochs))\n",
        "        print(\"Average Training Loss: {:.3f}\".format(total_train_loss/len(train_batches_text)))\n",
        "        print(\"Average Training Accuracy: {:.2f}\".format(100*total_train_acc/len(train_batches_text)))\n",
        "        print(\"Average Validation Loss: {:.3f}\".format(avg_val_loss))\n",
        "        print(\"Average Validation Accuracy: {:.2f}\".format(100*total_val_acc/len(val_batches_text)))\n",
        "              \n",
        "        if (avg_val_loss < best_loss):\n",
        "            best_loss = avg_val_loss\n",
        "            save_data={'best_loss':best_loss,'covered_epochs':covered_epochs+epoch+1}\n",
        "            impatience=0\n",
        "            with open('/content/Model_Backup/Seq2seq_summarization.pkl', 'wb') as fp:\n",
        "                pickle.dump(save_data, fp)\n",
        "            saver.save(sess, save_path='/content/Model_Backup/Seq2seq_summarization.ckpt')\n",
        "            print(\"\\nModel saved\\n\")\n",
        "              \n",
        "        else:\n",
        "            impatience+=1\n",
        "              \n",
        "        if impatience > patience:\n",
        "              break\n",
        "              \n",
        "              \n",
        "        epoch+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Load checkpoint? y/n: y\n",
            "\n",
            "Loading pre-trained weights for the model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-f48e2cfe0404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading pre-trained weights for the model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/Model_Backup/Seq2seq_summarization.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0mcheckpoint_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[1;32m   1282\u001b[0m                        checkpoint_prefix)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/checkpoint_management.py\u001b[0m in \u001b[0;36mcheckpoint_exists_internal\u001b[0;34m(checkpoint_prefix)\u001b[0m\n\u001b[1;32m    364\u001b[0m   pathname = _prefix_to_checkpoint_path(checkpoint_prefix,\n\u001b[1;32m    365\u001b[0m                                         saver_pb2.SaverDef.V2)\n\u001b[0;32m--> 366\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mfilesystem\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0mlisting\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \"\"\"\n\u001b[0;32m--> 363\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_matching_files_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         for matching_filename in pywrap_tensorflow.GetMatchingFiles(\n\u001b[0;32m--> 384\u001b[0;31m             compat.as_bytes(pattern))\n\u001b[0m\u001b[1;32m    385\u001b[0m     ]\n\u001b[1;32m    386\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /content/Model_Backup; No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bCz1tESrn9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7_zedAzsBpZ",
        "colab_type": "code",
        "outputId": "56372d58-4c03-46e2-c78a-2b599101c57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}